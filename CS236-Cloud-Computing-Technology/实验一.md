# 实验一：企业云实践应用

[总参考教程：spark的安装和使用](https://www.cnblogs.com/dion-90/articles/9058500.html)

------------------

## 配置`JAVA`环境

[ubuntu18.04安装jdk](https://blog.csdn.net/weixin_38883338/article/details/82079194)

&emsp; 使用`sudo vim ~/.bashrc`配置环境变量。

&emsp; 注意，重新分配弹性IP后再次SSH连接会出现`WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!`的报错，此时需要执行`vim ~/.ssh/known_hosts`，并清空相关IP地址下的内容。

-------------------

## 安装和使用`hadoop`

[Ubuntu18.04安装hadoop](https://blog.csdn.net/weixin_38883338/article/details/82928809)

[ubuntu18.04安装Hadoop（补充说明）](https://blog.csdn.net/weixin_42001089/article/details/81865101)

[主要：hadoop集群配置](https://medium.com/@jootorres_11979/how-to-set-up-a-hadoop-3-2-1-multi-node-cluster-on-ubuntu-18-04-2-nodes-567ca44a3b12)

&emsp; 查看所有`hadoop`示例（使用见总参考教程）：

```
./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar
```

&emsp; 注意，每次运行完需要将`./output`删除。

&emsp; 在运行`HDFS`时，将`start-dfs.sh`改为`./sbin/start-dfs.sh`

&emsp; 在`/usr/local/hadoop/test`目录下运行编译`javac test.java`时报错：

```
hadoop@hadoop-master:/usr/local/hadoop/test$ javac test.java
test.java:1: error: package org.apache.hadoop.io does not exist
import org.apache.hadoop.io.IntWritable;
                           ^
test.java:2: error: package org.apache.hadoop.io does not exist
import org.apache.hadoop.io.LongWritable;
                           ^
test.java:3: error: package org.apache.hadoop.io does not exist
import org.apache.hadoop.io.NullWritable;
                           ^
test.java:4: error: package org.apache.hadoop.io does not exist
import org.apache.hadoop.io.Text;
                           ^
test.java:5: error: package org.apache.hadoop.mapreduce does not exist
import org.apache.hadoop.mapreduce.Mapper;
                                  ^
5 errors
```

&emsp; 解决方案是改为`javac test.java -cp $(hadoop classpath)`

------------------

## 安装`Spark`

[Spark安装和集群搭建](https://medium.com/@jootorres_11979/how-to-install-and-set-up-an-apache-spark-cluster-on-hadoop-18-04-b4d70650ed42)

&emsp; `Spark shell`运行`.scala`程序：`:load [host path]/test.scala`。退出shell：`:quit`

-----------------

```
export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_271
export JRE_HOME=${JAVA_HOME}/jre  
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib  
export PATH=${JAVA_HOME}/bin:$PATH
```

```
sudo apt install ssh
sudo apt install pdsh
```

```
export PDSH_RCMD_TYPE=ssh
```

```
ssh-keygen -t rsa -P ""
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
ssh localhost
```
```
export JAVA_HOME=/usr/lib/jvm/jdk-1.8.0_271/
```

```
PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/
  games:/usr/local/games:/usr/local/hadoop/bin:/usr/local/hadoop/sbin$
JAVA_HOME="/usr/lib/jvm/jdk1.8.0_271/jre"
```
```
sudo usermod -aG hadoop hadoop
sudo chown hadoop:root -R /usr/local/hadoop/
sudo chmod g+rwx -R /usr/local/hadoop/
sudo adduser hadoop sudo
```

```
ssh-copy-id hadoop@hadoop-master
ssh-copy-id hadoop@hadoop-slave1
ssh-copy-id hadoop@hadoop-slave2
```

```
<configuration>
<property>
<name>fs.defaultFS</name>
<value>hdfs://hadoop-master:9000</value>
</property>
</configuration>
```

```
<configuration>
<property>
<name>dfs.namenode.name.dir</name><value>/usr/local/hadoop/data/nameNode</value>
</property>
<property>
<name>dfs.datanode.data.dir</name><value>/usr/local/hadoop/data/dataNode</value>
</property>
<property>
<name>dfs.replication</name>
<value>2</value>
</property>
</configuration>
```

```
scp /usr/local/hadoop/etc/hadoop/* hadoop-slave1:/usr/local/hadoop/etc/hadoop/
scp /usr/local/hadoop/etc/hadoop/* hadoop-slave2:/usr/local/hadoop/etc/hadoop/
```

```
source /etc/environment
hdfs namenode -format
```

```
export HADOOP_HOME="/usr/local/hadoop"
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
```

```
<property>
<name>yarn.resourcemanager.hostname</name>
<value>hadoop-master</value>
</property>
```

```
export PATH = $PATH:/usr/local/spark/bin
```


